name: FedProx Nightly Comparison

on:
  schedule:
    - cron: "0 2 * * *" # 2 AM UTC daily
  workflow_dispatch:
    inputs:
      alpha_values:
        description: 'Alpha values for non-IID testing (comma-separated, e.g., "0.05,0.1,0.5")'
        required: false
        default: "0.05,0.1,0.5"
        type: string
      mu_values:
        description: 'FedProx mu values (comma-separated, e.g., "0.0,0.01,0.1")'
        required: false
        default: "0.0,0.01,0.1"
        type: string
      client_count:
        description: "Number of clients for comparison"
        required: false
        default: "10"
        type: string
      round_count:
        description: "Number of rounds for comparison"
        required: false
        default: "20"
        type: string

permissions:
  contents: write
  actions: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  fedprox_comparison:
    runs-on: ubuntu-latest
    timeout-minutes: 240 # 4 hours
    strategy:
      fail-fast: false
      matrix:
        alpha: [0.05, 0.1, 0.5]
        mu: [0.0, 0.01, 0.1]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: pip-${{ runner.os }}-

      - name: Prepare real dataset samples
        run: python scripts/setup_real_datasets.py

      - name: Install dependencies with retry
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 10
          max_attempts: 3
          retry_on: error
          command: |
            pip install -r requirements.txt
            if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: FedProx Comparison (real UNSW)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 180
          max_attempts: 2
          retry_on: error
          command: |
            export D2_EXTENDED_METRICS="1"
            export ALPHA=${{ matrix.alpha }}
            export MU=${{ matrix.mu }}
            export ROUNDS=20
            export CLIENTS=6
            export ROUNDS=20
            export ALPHA="${{ matrix.alpha }}"
            export MU="${{ matrix.mu }}"
            PRESET="nightly_fedprox_alpha${ALPHA}_mu${MU}"
            DATASET="unsw"
            DATA_PATH="data/unsw/unsw_nb15_sample.csv"

            mkdir -p "runs/$PRESET"
            LOGDIR="runs/$PRESET"
            PORT=$((8200 + RANDOM % 1000))
            echo "$DATASET" >"$LOGDIR/dataset.txt"
            echo "$DATA_PATH" >"$LOGDIR/dataset_path.txt"
            echo "Running FedProx comparison with alpha=$ALPHA mu=$MU dataset=$DATASET"

            # Start server
            python server.py --rounds $ROUNDS --aggregation fedavg --server_address 127.0.0.1:$PORT --logdir "$LOGDIR" >"$LOGDIR/server.log" 2>&1 &
            SERVER_PID=$!

            # Wait for server
            for i in {1..20}; do
              nc -z 127.0.0.1 $PORT 2>/dev/null && break
              sleep 0.5
            done

            # Start all clients with FedProx mu parameter
            CLIENT_PIDS=()
            for CLIENT_ID in $(seq 0 $((CLIENTS-1))); do
              python client.py --server_address 127.0.0.1:$PORT --client_id $CLIENT_ID --num_clients $CLIENTS \
                --dataset "$DATASET" --data_path "$DATA_PATH" --partition_strategy dirichlet --alpha $ALPHA \
                --fedprox_mu $MU --logdir "$LOGDIR" --leakage_safe --seed 42 >"$LOGDIR/client_${CLIENT_ID}.log" 2>&1 &
              CLIENT_PIDS+=($!)
            done

            # Wait for all clients
            for PID in "${CLIENT_PIDS[@]}"; do
              wait $PID
            done

            # Clean up server
            sleep 2
            kill $SERVER_PID 2>/dev/null || true

      - name: Generate comparison plots
        run: |
          # Use same directory construction as training step
          export ALPHA="${{ matrix.alpha }}"
          export MU="${{ matrix.mu }}"
          PRESET="nightly_fedprox_alpha${ALPHA}_mu${MU}"
          RUN_DIR="runs/$PRESET"
          python scripts/plot_metrics.py \
            --run_dir "$RUN_DIR" \
            --output_dir "$RUN_DIR"
          if [ -f "$RUN_DIR/client_metrics_plot.png" ]; then
            cp "$RUN_DIR/client_metrics_plot.png" "$RUN_DIR/fedprox_comparison.png"
          fi

      - name: Generate thesis-quality plots
        run: |
          export ALPHA="${{ matrix.alpha }}"
          export MU="${{ matrix.mu }}"
          PRESET="nightly_fedprox_alpha${ALPHA}_mu${MU}"
          RUN_DIR="runs/$PRESET"

          # Generate heterogeneity dimension plots (alpha variation is heterogeneity)
          mkdir -p "$RUN_DIR/thesis_plots"
          python scripts/generate_thesis_plots.py \
            --dimension heterogeneity \
            --runs_dir runs \
            --output_dir "$RUN_DIR/thesis_plots" || echo "Thesis plots generation failed (non-fatal)"

      - name: Validate results
        run: |
          # Use non-strict FPR validation for real datasets (warnings only)
          # Real-world datasets like UNSW have complex distributions that make
          # strict FPR targets (Â±0.02 of 0.10) difficult to achieve
          python scripts/ci_checks.py --runs_dir runs

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fedprox-nightly-alpha${{ matrix.alpha }}-mu${{ matrix.mu }}-${{ github.sha }}
          path: |
            runs/**/metrics.csv
            runs/**/client_*_metrics.csv
            runs/**/fedprox_comparison.png
            runs/**/server.log
            runs/**/client_*.log
            runs/**/server_metrics_plot.png
            runs/**/client_metrics_plot.png
            runs/**/thesis_plots/**/*.png
            runs/**/thesis_plots/**/*.pdf
          retention-days: 30

  fedprox_summary:
    needs: fedprox_comparison
    runs-on: ubuntu-latest
    if: always()
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install pandas matplotlib seaborn numpy

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: nightly-artifacts

      - name: Generate FedProx comparison summary
        run: |
          python scripts/analyze_fedprox_comparison.py \
            --artifacts_dir nightly-artifacts \
            --output_dir fedprox-summary

      - name: Upload consolidated results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fedprox-nightly-summary-${{ github.sha }}
          path: |
            fedprox-summary/fedprox_comparison_summary.json
            fedprox-summary/fedprox_performance_plots.png
            fedprox-summary/fedprox_thesis_tables.tex
            fedprox-summary/thesis_plots/**/*.png
            fedprox-summary/thesis_plots/**/*.pdf
          retention-days: 60

      - name: Locate baseline summary run
        id: find_baseline
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const runs = await github.rest.actions.listWorkflowRuns({
              owner,
              repo,
              workflow_id: 'fedprox-nightly.yml',
              branch: 'main',
              status: 'success',
              per_page: 10,
            });
            const currentRunId = context.runId;
            const baseline = runs.data.workflow_runs.find(run => run.id !== currentRunId);
            if (!baseline) {
              core.info('No prior successful main run found; skipping regression comparison.');
              core.setOutput('run_id', '');
              core.setOutput('artifact_name', '');
              return;
            }

            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner,
              repo,
              run_id: baseline.id,
              per_page: 20,
            });

            const summary = artifacts.data.artifacts.find(artifact => artifact.name.startsWith('fedprox-nightly-summary-'));
            if (!summary) {
              core.info('Prior successful run did not publish a summary artifact.');
              core.setOutput('run_id', '');
              core.setOutput('artifact_name', '');
              return;
            }

            core.info(`Using baseline run ${baseline.id} artifact ${summary.name}`);
            core.setOutput('run_id', String(baseline.id));
            core.setOutput('artifact_name', summary.name);

      - name: Download baseline summary artifact
        if: steps.find_baseline.outputs.artifact_name != ''
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ steps.find_baseline.outputs.run_id }}
          name: ${{ steps.find_baseline.outputs.artifact_name }}
          path: baseline-summary

      - name: Compare summary against baseline
        id: regression_check
        run: |
          set -euo pipefail
          mkdir -p regression-report
          if [ ! -f fedprox-summary/fedprox_comparison_summary.json ]; then
            echo "Current summary missing; skipping regression check." > regression-report/report.txt
            echo "baseline_missing=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          if [ "${{ steps.find_baseline.outputs.artifact_name }}" = "" ]; then
            echo "Baseline summary unavailable; regression checks skipped." > regression-report/report.txt
            echo "baseline_missing=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          BASELINE_FILE=$(find baseline-summary -name 'fedprox_comparison_summary.json' -print -quit)
          if [ -z "$BASELINE_FILE" ]; then
            echo "Baseline summary artifact missing expected JSON; regression checks skipped." > regression-report/report.txt
            echo "baseline_missing=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          python scripts/check_regressions.py \
            --baseline "$BASELINE_FILE" \
            --candidate fedprox-summary/fedprox_comparison_summary.json \
            --output regression-report/regressions.json | tee regression-report/report.txt
          echo "baseline_missing=false" >> "$GITHUB_OUTPUT"

      - name: Append regression report to summary
        if: always()
        run: |
          if [ -f regression-report/report.txt ]; then
            {
              echo '### Regression Check'
              echo ''
              cat regression-report/report.txt
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fedprox-regression-report-${{ github.sha }}
          path: regression-report
          retention-days: 30

  commit_plots:
    needs: [fedprox_summary]
    runs-on: ubuntu-latest
    if: always() && needs.fedprox_summary.result == 'success'
    steps:
      - uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download consolidated results
        uses: actions/download-artifact@v4
        with:
          name: fedprox-nightly-summary-${{ github.sha }}
          path: fedprox-summary

      - name: Download manual comparison results
        uses: actions/download-artifact@v4
        with:
          name: manual-fedprox-comparison-${{ github.sha }}
          path: manual-comparison
        continue-on-error: true

      - name: Commit plots to repository
        run: |
          # Configure git
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"

          # Create plots directory if it doesn't exist
          mkdir -p plots

          # Copy consolidated summary plots
          if [ -d "fedprox-summary" ]; then
            python scripts/commit_plots.py \
              --source_dir fedprox-summary \
              --experiment_type fedprox-nightly \
              --plots_dir plots
          fi

          # Copy manual comparison plots if available
          if [ -d "manual-comparison" ]; then
            python scripts/commit_plots.py \
              --source_dir manual-comparison \
              --experiment_type fedprox-manual \
              --plots_dir plots
          fi

      - name: Push plots to repository
        run: |
          # Check if there are unpushed commits
          if git log origin/main..HEAD --oneline | grep -q .; then
            git push origin main
          else
            echo "No commits to push"
          fi

  fedprox_notify:
    needs: fedprox_comparison
    runs-on: ubuntu-latest
    if: always() && needs.fedprox_comparison.result == 'failure'
    steps:
      - name: Log FedProx nightly failure
        run: |
          echo "ð¨ FedProx Nightly Comparison Failed"
          echo "Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo "Branch: ${{ github.ref }}"
          echo "Commit: ${{ github.sha }}"
          echo "Failed at: $(date)"
          echo "Check the workflow run for details"

  manual_fedprox_comparison:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: pip-${{ runner.os }}-

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Parse parameters and run FedProx comparison matrix
        env:
          D2_EXTENDED_METRICS: "1"
        run: |
          IFS=',' read -ra ALPHAS <<< "${{ github.event.inputs.alpha_values }}"
          IFS=',' read -ra MUS <<< "${{ github.event.inputs.mu_values }}"

          for alpha in "${ALPHAS[@]}"; do
            alpha=$(echo "$alpha" | xargs)  # trim whitespace
            for mu in "${MUS[@]}"; do
              mu=$(echo "$mu" | xargs)  # trim whitespace
              echo "Running FedProx comparison: alpha=${alpha}, mu=${mu}"

              CLIENTS=${{ github.event.inputs.client_count }}
              ROUNDS=${{ github.event.inputs.round_count }}
              PRESET="manual_fedprox_alpha${alpha}_mu${mu}"
              
              mkdir -p "runs/$PRESET"
              LOGDIR="runs/$PRESET"
              PORT=$((9200 + RANDOM % 1000))
              
              # Start server
              python server.py --rounds $ROUNDS --aggregation fedavg --server_address 127.0.0.1:$PORT --logdir "$LOGDIR" >"$LOGDIR/server.log" 2>&1 &
              SERVER_PID=$!
              
              # Wait for server
              for i in {1..20}; do
                nc -z 127.0.0.1 $PORT 2>/dev/null && break
                sleep 0.5
              done
              
              # Start all clients with FedProx mu parameter
              CLIENT_PIDS=()
              for CLIENT_ID in $(seq 0 $((CLIENTS-1))); do
                python client.py --server_address 127.0.0.1:$PORT --client_id $CLIENT_ID --num_clients $CLIENTS \
                  --dataset synthetic --samples 1000 --features 20 --partition_strategy dirichlet --alpha "$alpha" \
                  --fedprox_mu "$mu" --logdir "$LOGDIR" --leakage_safe --seed 42 >"$LOGDIR/client_${CLIENT_ID}.log" 2>&1 &
                CLIENT_PIDS+=($!)
              done
              
              # Wait for all clients
              for PID in "${CLIENT_PIDS[@]}"; do
                wait $PID
              done
              
              # Clean up server
              sleep 2
              kill $SERVER_PID 2>/dev/null || true

              python scripts/plot_metrics.py \
                --run_dir "runs/$PRESET" \
                --output_dir "runs/$PRESET"
              if [ -f "runs/$PRESET/client_metrics_plot.png" ]; then
                cp "runs/$PRESET/client_metrics_plot.png" "runs/$PRESET/comparison.png"
              fi
            done
          done

      - name: Validate all results
        run: |
          # Check that metrics files were created
          FOUND=0
          for dir in runs/*/; do
            if [ -f "$dir/metrics.csv" ]; then
              echo "â Found metrics in $(basename $dir)"
              FOUND=$((FOUND + 1))
            fi
          done
          echo "Total experiments with metrics: $FOUND"
          if [ $FOUND -eq 0 ]; then
            echo "Error: No experiments produced metrics"
            exit 1
          fi

      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: manual-fedprox-comparison-${{ github.sha }}
          path: runs/**
          include-hidden-files: false
          retention-days: 14
