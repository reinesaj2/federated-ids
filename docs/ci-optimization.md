# CI Optimization and Timeout Parameters

## Overview

This document describes the timeout configuration for federated learning experiments, both for local execution and CI/CD workflows. These optimizations were implemented to handle full-dataset experiments (82k samples) and prevent resource exhaustion.

## Timeout Parameters

### Script-Level Timeouts

The `scripts/comparative_analysis.py` script accepts two timeout parameters:

#### Server Timeout

```bash
--server_timeout <seconds>
```

- **Default**: 300 seconds (5 minutes)
- **Purpose**: Maximum time allowed for the Flower server process
- **When to increase**: Full dataset experiments, complex aggregation methods (Bulyan, Krum)

#### Client Timeout

```bash
--client_timeout <seconds>
```

- **Default**: 900 seconds (15 minutes)
- **Purpose**: Maximum time allowed for each client training process
- **When to increase**: Large datasets, deep models, differential privacy experiments

### CI Workflow Timeouts

File: `.github/workflows/comparative-analysis-nightly.yml`

```yaml
jobs:
  comparative_analysis:
    timeout-minutes: 480 # 8 hours
    strategy:
      max-parallel: 2 # Limit concurrent experiments
```

- **Workflow timeout**: 480 minutes (8 hours) for complete experiment matrix
- **Parallel jobs**: Maximum 2 concurrent experiments to prevent resource exhaustion
- **Success threshold**: 50% of experiments must complete successfully

## Usage Examples

### Local Execution

#### Default timeouts (sampled dataset)

```bash
python scripts/comparative_analysis.py \
  --alpha 0.5 \
  --adv_fraction 0
```

#### Extended timeouts (full dataset)

```bash
python scripts/comparative_analysis.py \
  --alpha 0.5 \
  --adv_fraction 0 \
  --server_timeout 600 \
  --client_timeout 1800
```

#### Custom timeouts for specific scenarios

```bash
# Byzantine experiments with full dataset
python scripts/comparative_analysis.py \
  --alpha 0.5 \
  --adv_fraction 0.3 \
  --server_timeout 900 \
  --client_timeout 2400

# Differential privacy experiments
python scripts/comparative_analysis.py \
  --alpha 0.5 \
  --dp_epsilon 1.0 \
  --server_timeout 450 \
  --client_timeout 1200
```

### CI Configuration

The nightly workflow automatically uses appropriate timeouts:

```yaml
env:
  SERVER_TIMEOUT: 300
  CLIENT_TIMEOUT: 900
```

Pass to script:

```bash
python scripts/comparative_analysis.py \
  --server_timeout ${SERVER_TIMEOUT} \
  --client_timeout ${CLIENT_TIMEOUT}
```

## Timeout Selection Guidelines

### Server Timeout

| Scenario                       | Recommended Value | Rationale                                 |
| ------------------------------ | ----------------- | ----------------------------------------- |
| Sampled dataset (8.2k samples) | 300s (default)    | Fast aggregation, minimal overhead        |
| Full dataset (82k samples)     | 600s              | 10x data requires longer aggregation      |
| Bulyan/Krum aggregation        | 900s              | O(nÂ²) complexity for distance computation |
| Byzantine experiments          | 900s              | Additional validation overhead            |

### Client Timeout

| Scenario             | Recommended Value | Rationale                          |
| -------------------- | ----------------- | ---------------------------------- |
| Sampled dataset      | 900s (default)    | 3 local epochs on 8.2k samples     |
| Full dataset         | 1800s             | 10x data, 3 local epochs           |
| Differential privacy | 1200s             | Noise injection adds ~30% overhead |
| Personalization      | 1500s             | Additional fine-tuning phase       |

## Error Handling

### Timeout Behavior

When a timeout occurs:

1. **Server timeout**: Process terminated, experiment marked as failed
2. **Client timeout**: Individual client terminated, server waits for remaining clients
3. **Workflow timeout**: Entire CI job terminated, partial results preserved

### Graceful Degradation

The CI workflow uses a 50% success threshold:

```bash
# Validation logic
if [ $FOUND -lt $((TOTAL / 2)) ]; then
  echo "WARNING: Low success rate, but continuing..."
else
  echo "SUCCESS: Adequate experiment completion rate"
fi
```

This allows partial failures without blocking the entire pipeline.

## Performance Monitoring

### Logged Metrics

Each experiment logs timing information to `metrics.csv`:

- `t_aggregate_ms`: Aggregation time at server
- `t_round_ms`: Total round time (training + aggregation)
- `t_fit_ms`: Client training time

### Identifying Timeout Issues

Check experiment logs for timeout patterns:

```bash
# Find timed-out experiments
grep -r "TimeoutExpired" runs/*/experiment.log

# Check average round times
awk -F',' 'NR>1 {sum+=$11; count++} END {print "Avg round time:", sum/count/1000, "seconds"}' \
  runs/*/metrics.csv
```

## Historical Context

### Pre-Optimization Issues

Before parameterization:

- Fixed 120s server timeout caused failures on full dataset
- Fixed 600s client timeout insufficient for DP experiments
- No graceful degradation in CI
- Resource exhaustion from unlimited parallel jobs

### Implementation (PR #91)

Changes merged 2025-10-21:

- Parameterized timeouts in `comparative_analysis.py`
- Extended workflow timeout to 8 hours
- Limited parallel jobs to 2
- Added 50% success threshold
- Proper error logging and cleanup

## Troubleshooting

### Experiment Times Out Despite High Timeout

**Check**:

1. System resources (CPU, memory, disk I/O)
2. Port conflicts (default 8080)
3. Dataset corruption or missing files

**Solution**:

```bash
# Monitor resources during experiment
python scripts/comparative_analysis.py --alpha 0.5 &
PID=$!
while kill -0 $PID 2>/dev/null; do
  ps -p $PID -o %cpu,%mem,etime
  sleep 30
done
```

### CI Workflow Exceeds 8 Hours

**Check**:

1. Number of experiments in matrix
2. Dataset size setting
3. Number of rounds configured

**Solution**: Reduce experiment scope or split into multiple workflows

### High Failure Rate in CI

**Check**:

1. Success rate from validation logs
2. Individual experiment error logs
3. Timeout values vs actual execution times

**Solution**: Adjust timeouts based on `t_round_ms` percentiles from successful runs

## Related Documentation

- `EXPERIMENT_CONSTRAINTS.md`: Mathematical constraints for Bulyan/Krum
- `docs/gradient_clipping_theory.md`: Byzantine attack mitigations
- `docs/threat_model.md`: Security considerations for federated IDS
- `.github/workflows/comparative-analysis-nightly.yml`: CI configuration
